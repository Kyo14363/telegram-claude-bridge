#!/usr/bin/env python3
"""
Telegram <-> Claude Code Bridge with Context Memory v2.4
========================================================
- å°è©±æ­·å²ç¶­è­·æ©Ÿåˆ¶
- æ¯æ—¥ç¨ç«‹ log æª”æ¡ˆ
- è‡ªå‹•æ¸…ç† 14 å¤©å‰çš„èˆŠ log
- URL é è™•ç†ï¼šè‡ªå‹•æŠ“å–é€£çµå…§å®¹ï¼ˆfxtwitter ç‚ºä¸»ï¼Œyt-dlp ç‚ºè¼”ï¼‰
"""

import os
import sys
import json
import asyncio
import subprocess
import re
import glob
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass, field, asdict
import logging
from logging.handlers import TimedRotatingFileHandler
from urllib.parse import urlparse

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# === Configuration ===
BASE_DIR = Path(__file__).parent.resolve()

CONFIG = {
    "TELEGRAM_BOT_TOKEN": os.getenv("TELEGRAM_BOT_TOKEN", ""),
    "ALLOWED_USER_IDS": [int(x) for x in os.getenv("ALLOWED_USER_ID", "").split(",") if x.strip().isdigit()],
    "CLAUDE_CLI": os.getenv("CLAUDE_CLI_PATH", os.path.expandvars(r"%APPDATA%\\npm\\claude.cmd")),
    "WORKING_DIR": Path(os.getenv("WORKING_DIR", str(Path.home() / "claude-workspace"))),
    "BASE_DIR": BASE_DIR,
    "HISTORY_FILE": BASE_DIR / "conversation_history.json",
    "LOG_DIR": BASE_DIR / "logs",
    "TIMEOUT": int(os.getenv("TIMEOUT", "300")),
    "MAX_HISTORY_ROUNDS": int(os.getenv("MAX_HISTORY_ROUNDS", "10")),
    "ALLOW_DANGEROUS": os.getenv("ALLOW_DANGEROUS", "false").lower() == "true",
    "LOG_RETENTION_DAYS": int(os.getenv("LOG_RETENTION_DAYS", "14")),
    "URL_FETCH_TIMEOUT": int(os.getenv("URL_FETCH_TIMEOUT", "15")),
    "FETCH_OUTPUT_DIR": BASE_DIR / "fetch_outputs",
}

CONFIG["WORKING_DIR"].mkdir(parents=True, exist_ok=True)
CONFIG["LOG_DIR"].mkdir(parents=True, exist_ok=True)
CONFIG["FETCH_OUTPUT_DIR"].mkdir(parents=True, exist_ok=True)

# === æ—¥èªŒè¨­å®šï¼ˆæ¯æ—¥è¼ªæ›ï¼‰===
def setup_logging():
    """è¨­å®šæ¯æ—¥è¼ªæ›çš„æ—¥èªŒç³»çµ±"""
    log_file = CONFIG["LOG_DIR"] / "bridge.log"
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    file_handler = TimedRotatingFileHandler(
        log_file, when='midnight', interval=1,
        backupCount=CONFIG["LOG_RETENTION_DAYS"], encoding='utf-8'
    )
    file_handler.suffix = "%Y-%m-%d.log"
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logging.getLogger(__name__)

def cleanup_old_logs():
    """æ¸…ç†è¶…éä¿ç•™å¤©æ•¸çš„èˆŠ log æª”æ¡ˆ"""
    cutoff_date = datetime.now() - timedelta(days=CONFIG["LOG_RETENTION_DAYS"])
    log_pattern = CONFIG["LOG_DIR"] / "bridge.log.*"
    deleted_count = 0
    for log_file in glob.glob(str(log_pattern)):
        try:
            date_str = log_file.split('.')[-1].replace('.log', '')
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
            if file_date < cutoff_date:
                os.remove(log_file)
                deleted_count += 1
        except (ValueError, OSError):
            continue
    if deleted_count > 0:
        logging.info(f"å·²æ¸…ç† {deleted_count} å€‹è¶…é {CONFIG['LOG_RETENTION_DAYS']} å¤©çš„èˆŠ log æª”æ¡ˆ")

logger = setup_logging()

try:
    from telegram import Update
    from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
    TELEGRAM_LIB_AVAILABLE = True
except ImportError:
    TELEGRAM_LIB_AVAILABLE = False

# === URL é è™•ç†æ¨¡çµ„ ===

# æª¢æŸ¥ yt-dlp æ˜¯å¦å¯ç”¨
try:
    import yt_dlp
    YTDLP_AVAILABLE = True
    logger.info("yt-dlp å¯ç”¨ï¼Œå·²å•Ÿç”¨ä½œç‚ºå‚™ç”¨ URL è™•ç†å™¨")
except ImportError:
    YTDLP_AVAILABLE = False
    logger.info("yt-dlp æœªå®‰è£ï¼Œåƒ…ä½¿ç”¨ fxtwitter/HTTP æ–¹æ¡ˆè™•ç† URL")

# æª¢æŸ¥ requests æ˜¯å¦å¯ç”¨
# --- LangExtract ---
try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    LANGEXTRACT_AVAILABLE = False
    lx = None

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    logger.warning("requests æœªå®‰è£ï¼ŒURL é è™•ç†åŠŸèƒ½å°‡å—é™")


# --- URL åµæ¸¬èˆ‡åˆ†é¡ ---

# æ”¯æ´çš„å¹³å° domain å°æ‡‰
PLATFORM_PATTERNS = {
    "x_twitter": [
        r"(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/\S+",
        r"(?:https?://)?t\.co/\S+",
    ],
    "youtube": [
        r"(?:https?://)?(?:www\.)?youtube\.com/watch\S+",
        r"(?:https?://)?youtu\.be/\S+",
        r"(?:https?://)?(?:www\.)?youtube\.com/shorts/\S+",
    ],
    "general": [
        r"https?://\S+",
    ],
}

def detect_urls(text: str) -> List[Tuple[str, str]]:
    """
    å¾è¨Šæ¯ä¸­åµæ¸¬ URL ä¸¦åˆ†é¡å¹³å°ã€‚
    å›å‚³ [(url, platform), ...] çš„åˆ—è¡¨ã€‚
    å„ªå…ˆåŒ¹é…ç‰¹å®šå¹³å°ï¼Œæœ€å¾Œæ‰åŒ¹é… generalã€‚
    """
    found = []
    found_urls = set()
    
    # å…ˆåŒ¹é…ç‰¹å®šå¹³å°
    for platform in ["x_twitter", "youtube"]:
        for pattern in PLATFORM_PATTERNS[platform]:
            for match in re.finditer(pattern, text):
                url = match.group(0)
                if url not in found_urls:
                    found_urls.add(url)
                    found.append((url, platform))
    
    # å†åŒ¹é…å…¶ä»–æ‰€æœ‰ URL
    for pattern in PLATFORM_PATTERNS["general"]:
        for match in re.finditer(pattern, text):
            url = match.group(0)
            if url not in found_urls:
                found_urls.add(url)
                found.append((url, "general"))
    
    return found


# --- æ–¹æ¡ˆ D: fxtwitter (X/Twitter å°ˆç”¨) ---

def fetch_via_fxtwitter(url: str) -> Optional[str]:
    """
    ç”¨ fxtwitter.com API æŠ“å– X/Twitter æ¨æ–‡å…§å®¹ã€‚
    å°‡ x.com / twitter.com æ›¿æ›æˆ api.fxtwitter.com å–å¾— JSONã€‚
    """
    if not REQUESTS_AVAILABLE:
        return None
    
    try:
        # å°‡ URL è½‰æˆ fxtwitter API æ ¼å¼
        api_url = re.sub(
            r"https?://(www\.)?(twitter\.com|x\.com)",
            "https://api.fxtwitter.com",
            url
        )
        
        logger.info(f"[fxtwitter] å˜—è©¦æŠ“å–: {api_url}")
        
        resp = requests.get(api_url, timeout=CONFIG["URL_FETCH_TIMEOUT"], headers={
            "User-Agent": "TelegramClaudeBridge/2.2"
        })
        
        if resp.status_code != 200:
            logger.warning(f"[fxtwitter] HTTP {resp.status_code}")
            return None
        
        data = resp.json()
        tweet = data.get("tweet", {})
        
        if not tweet:
            logger.warning("[fxtwitter] å›æ‡‰ä¸­ç„¡ tweet è³‡æ–™")
            return None
        
        # çµ„è£æ¨æ–‡å…§å®¹
        parts = []
        parts.append(f"ğŸ“Œ æ¨æ–‡ä¾†æº: {url}")
        
        author = tweet.get("author", {})
        if author:
            parts.append(f"ğŸ‘¤ ä½œè€…: {author.get('name', '?')} (@{author.get('screen_name', '?')})")
        
        text = tweet.get("text", "")
        if text:
            parts.append(f"ğŸ“ å…§å®¹:\n{text}")
        
        # åª’é«”è³‡è¨Š
        media = tweet.get("media", {})
        if media:
            photos = media.get("photos", [])
            videos = media.get("videos", [])
            if photos:
                parts.append(f"ğŸ–¼ï¸ åŒ…å« {len(photos)} å¼µåœ–ç‰‡")
            if videos:
                parts.append(f"ğŸ¬ åŒ…å« {len(videos)} å€‹å½±ç‰‡")
        
        # äº’å‹•æ•¸æ“š
        likes = tweet.get("likes", 0)
        retweets = tweet.get("retweets", 0)
        replies = tweet.get("replies", 0)
        if likes or retweets or replies:
            parts.append(f"ğŸ’¬ äº’å‹•: {likes} è®š / {retweets} è½‰æ¨ / {replies} å›è¦†")
        
        created = tweet.get("created_at", "")
        if created:
            parts.append(f"ğŸ“… ç™¼å¸ƒæ™‚é–“: {created}")
        
        # å¼•ç”¨æ¨æ–‡
        quote = tweet.get("quote", {})
        if quote:
            quote_author = quote.get("author", {})
            quote_text = quote.get("text", "")
            parts.append(f"\nâ†©ï¸ å¼•ç”¨æ¨æ–‡ (@{quote_author.get('screen_name', '?')}):\n{quote_text}")
        
        result = "\n".join(parts)
        logger.info(f"[fxtwitter] æˆåŠŸæŠ“å–æ¨æ–‡ï¼Œ{len(result)} å­—å…ƒ")
        return result
        
    except requests.Timeout:
        logger.warning(f"[fxtwitter] è«‹æ±‚è¶…æ™‚")
        return None
    except Exception as e:
        logger.error(f"[fxtwitter] éŒ¯èª¤: {e}")
        return None


# --- æ–¹æ¡ˆ C: yt-dlp (é€šç”¨å‚™ç”¨) ---

def fetch_via_ytdlp(url: str) -> Optional[str]:
    """
    ç”¨ yt-dlp æå– URL çš„ metadataï¼ˆä¸ä¸‹è¼‰æª”æ¡ˆï¼‰ã€‚
    æ”¯æ´ X/Twitterã€YouTubeã€TikTok ç­‰ä¸Šåƒå€‹å¹³å°ã€‚
    """
    if not YTDLP_AVAILABLE:
        return None
    
    try:
        logger.info(f"[yt-dlp] å˜—è©¦æŠ“å–: {url}")
        
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'skip_download': True,
            'socket_timeout': CONFIG["URL_FETCH_TIMEOUT"],
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
        
        if not info:
            return None
        
        # çµ„è£ metadata
        parts = []
        parts.append(f"ğŸ”— ä¾†æº: {url}")
        
        title = info.get('title')
        if title:
            parts.append(f"ğŸ“Œ æ¨™é¡Œ: {title}")
        
        uploader = info.get('uploader') or info.get('channel')
        if uploader:
            parts.append(f"ğŸ‘¤ ä½œè€…/é »é“: {uploader}")
        
        description = info.get('description', '')
        if description:
            # é™åˆ¶æè¿°é•·åº¦é¿å…å¤ªé•·
            desc_preview = description[:1000]
            if len(description) > 1000:
                desc_preview += "...(å·²æˆªæ–·)"
            parts.append(f"ğŸ“ æè¿°/å…§å®¹:\n{desc_preview}")
        
        duration = info.get('duration')
        if duration:
            mins, secs = divmod(int(duration), 60)
            hours, mins = divmod(mins, 60)
            if hours:
                parts.append(f"â±ï¸ æ™‚é•·: {hours}:{mins:02d}:{secs:02d}")
            else:
                parts.append(f"â±ï¸ æ™‚é•·: {mins}:{secs:02d}")
        
        view_count = info.get('view_count')
        like_count = info.get('like_count')
        if view_count or like_count:
            stats = []
            if view_count:
                stats.append(f"{view_count:,} è§€çœ‹")
            if like_count:
                stats.append(f"{like_count:,} è®š")
            parts.append(f"ğŸ“Š æ•¸æ“š: {' / '.join(stats)}")
        
        upload_date = info.get('upload_date')
        if upload_date:
            try:
                formatted = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:]}"
                parts.append(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸ: {formatted}")
            except:
                pass
        
        # å¦‚æœæœ‰å­—å¹•/è‡ªå‹•å­—å¹•ï¼Œæç¤ºå¯ç”¨
        subtitles = info.get('subtitles', {})
        auto_subs = info.get('automatic_captions', {})
        if subtitles or auto_subs:
            langs = list(subtitles.keys()) + list(auto_subs.keys())
            parts.append(f"ğŸ’¬ å¯ç”¨å­—å¹•èªè¨€: {', '.join(langs[:10])}")
        
        result = "\n".join(parts)
        logger.info(f"[yt-dlp] æˆåŠŸæŠ“å– metadataï¼Œ{len(result)} å­—å…ƒ")
        return result
        
    except Exception as e:
        logger.error(f"[yt-dlp] éŒ¯èª¤: {e}")
        return None


# --- æ–¹æ¡ˆ fallback: åŸºæœ¬ HTTP æŠ“å– ---

def fetch_via_http(url: str) -> Optional[str]:
    """
    åŸºæœ¬ HTTP GETï¼Œå˜—è©¦æŠ“å–é é¢æ¨™é¡Œå’Œ meta descriptionã€‚
    ä½œç‚ºæœ€å¾Œçš„ fallbackã€‚
    """
    if not REQUESTS_AVAILABLE:
        return None
    
    try:
        logger.info(f"[http] å˜—è©¦æŠ“å–: {url}")
        
        resp = requests.get(url, timeout=CONFIG["URL_FETCH_TIMEOUT"], headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }, allow_redirects=True)
        
        if resp.status_code != 200:
            return None
        
        content = resp.text[:10000]  # åªå–å‰ 10K
        
        parts = [f"ğŸ”— ä¾†æº: {url}"]
        
        # æå– <title>
        title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
        if title_match:
            title = re.sub(r'\s+', ' ', title_match.group(1)).strip()
            parts.append(f"ğŸ“Œ æ¨™é¡Œ: {title}")
        
        # æå– og:title, og:description
        og_title = re.search(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)
        og_desc = re.search(r'<meta[^>]*property=["\']og:description["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)
        meta_desc = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)
        
        if og_title:
            parts.append(f"ğŸ“Œ OG æ¨™é¡Œ: {og_title.group(1)}")
        
        desc = (og_desc and og_desc.group(1)) or (meta_desc and meta_desc.group(1))
        if desc:
            parts.append(f"ğŸ“ æè¿°: {desc}")
        
        if len(parts) <= 1:
            # å¹¾ä¹ä»€éº¼éƒ½æ²’æŠ“åˆ°
            return None
        
        result = "\n".join(parts)
        logger.info(f"[http] æˆåŠŸæŠ“å–åŸºæœ¬è³‡è¨Šï¼Œ{len(result)} å­—å…ƒ")
        return result
        
    except Exception as e:
        logger.error(f"[http] éŒ¯èª¤: {e}")
        return None


# --- URL é è™•ç†èª¿åº¦å™¨ ---


def enhance_with_langextract(raw_content, url):
    # Use LE to extract structured info from fetched web content
    if not LANGEXTRACT_AVAILABLE or len(raw_content) < 200:
        return None
    try:
        import os
        if not os.getenv('GOOGLE_API_KEY'):
            return None
        logger.info(f'[langextract] extracting ({len(raw_content)} chars)...')
        extract_results = lx.extract(
            text=raw_content[:5000],
            prompt='Extract key information: main topic, key claims/data, people/orgs, numbers/stats, conclusion.',
            model='gemini-2.0-flash'
        )
        if not extract_results:
            return None
        result_text = str(extract_results)
        if len(result_text) < 50:
            return None
        sep = chr(10) + chr(10)
        return raw_content + sep + '=== LangExtract ===' + chr(10) + result_text[:2000] + chr(10) + '=== end ==='
    except Exception as e:
        logger.error(f'[langextract] failed: {e}')
        return None


def extract_structured_data(text, prompt=None):
    # /extract command: structured extraction on any text
    if not LANGEXTRACT_AVAILABLE:
        return 'LangExtract not installed'
    import os
    if not os.getenv('GOOGLE_API_KEY'):
        return 'GOOGLE_API_KEY not set'
    try:
        dp = 'Extract all key entities, facts, numbers, relationships. Organize in structured format.'
        res = lx.extract(text=text[:8000], prompt=prompt or dp, model='gemini-2.0-flash')
        if res:
            return 'LangExtract result:' + chr(10) + chr(10) + str(res)[:3000]
        return 'Extraction complete but no results'
    except Exception as e:
        return f'Extraction failed: {e}'



def save_fetch_output(url, fetched_content, claude_response, user_note=""):
    # Save AI-friendly markdown summary to fetch_outputs/
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_url = re.sub(r"[^a-zA-Z0-9]", "_", url[:60])
        filename = f"fetch_{ts}_{safe_url}.md"
        filepath = CONFIG["FETCH_OUTPUT_DIR"] / filename
        sep = chr(10)
        parts = []
        parts.append("# AI-Friendly Content Summary")
        parts.append("")
        parts.append(f"- **Source**: {url}")
        parts.append(f"- **Fetched**: {datetime.now().isoformat()}")
        if user_note:
            parts.append(f"- **User Note**: {user_note}")
        parts.append("")
        parts.append("---")
        parts.append("")
        parts.append("## Fetched Content")
        parts.append("")
        parts.append(fetched_content)
        parts.append("")
        parts.append("---")
        parts.append("")
        parts.append("## Claude Analysis")
        parts.append("")
        parts.append(claude_response)
        content = sep.join(parts)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        logger.info(f"[fetch] Saved: {filepath} ({len(content)} chars)")
        return str(filepath)
    except Exception as e:
        logger.error(f"[fetch] Save failed: {e}")
        return None


async def preprocess_urls(text: str) -> Tuple[str, List[str]]:
    """
    åµæ¸¬è¨Šæ¯ä¸­çš„ URLï¼Œè‡ªå‹•æŠ“å–å…§å®¹ï¼Œå›å‚³å¢å¼·å¾Œçš„è¨Šæ¯ã€‚
    
    ç­–ç•¥ï¼š
    - X/Twitter: fxtwitter (æ–¹æ¡ˆD) â†’ yt-dlp (æ–¹æ¡ˆC) â†’ http fallback
    - YouTube/å…¶ä»– yt-dlp æ”¯æ´å¹³å°: yt-dlp (æ–¹æ¡ˆC) â†’ http fallback
    - å…¶ä»– URL: http fallback
    
    å›å‚³: (å¢å¼·å¾Œçš„å®Œæ•´è¨Šæ¯, è™•ç†æ‘˜è¦åˆ—è¡¨)
    """
    urls = detect_urls(text)
    
    if not urls:
        return text, []
    
    logger.info(f"åµæ¸¬åˆ° {len(urls)} å€‹ URL: {urls}")
    
    enrichments = []
    summaries = []
    
    for url, platform in urls:
        content = None
        method_used = None
        
        if platform == "x_twitter":
            # X/Twitter: D â†’ C â†’ http
            content = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_fxtwitter, url
            )
            if content:
                method_used = "fxtwitter"
            else:
                content = await asyncio.get_event_loop().run_in_executor(
                    None, fetch_via_ytdlp, url
                )
                if content:
                    method_used = "yt-dlp"
        
        elif platform == "youtube":
            # YouTube: C â†’ http
            content = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_ytdlp, url
            )
            if content:
                method_used = "yt-dlp"
        
        # é€šç”¨ fallback
        if not content:
            content = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_http, url
            )
            if content:
                method_used = "http"
        
        if content:
            # LangExtract enhancement for general URLs
            if platform == "general" and LANGEXTRACT_AVAILABLE and len(content) > 300:
                enhanced = await asyncio.get_event_loop().run_in_executor(None, enhance_with_langextract, content, url)
                if enhanced:
                    content = enhanced
                    method_used = f"{method_used}+LE"
            enrichments.append(content)
            summaries.append(f"âœ… {url} â†’ {method_used}")
            logger.info(f"URL è™•ç†æˆåŠŸ: {url} via {method_used}")
        else:
            summaries.append(f"âš ï¸ {url} â†’ ç„¡æ³•æŠ“å–")
            logger.warning(f"URL è™•ç†å¤±æ•—: {url}")
    
    # çµ„è£å¢å¼·è¨Šæ¯
    if enrichments:
        enriched_block = "\n\n---\n".join(enrichments)
        enhanced_text = (
            f"{text}\n\n"
            f"=== ä»¥ä¸‹æ˜¯è‡ªå‹•æŠ“å–çš„é€£çµå…§å®¹ ===\n\n"
            f"{enriched_block}\n\n"
            f"=== é€£çµå…§å®¹çµæŸ ===\n"
            f"è«‹åŸºæ–¼ä¸Šè¿°é€£çµå…§å®¹ä¾†å›æ‡‰ä½¿ç”¨è€…çš„è¨Šæ¯ã€‚"
        )
        return enhanced_text, summaries
    
    return text, summaries


# === å°è©±æ­·å² ===

@dataclass
class Message:
    role: str
    content: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

class ConversationHistory:
    def __init__(self, max_rounds: int = 10):
        self.max_messages = max_rounds * 2
        self.messages: List[Message] = []
    
    def add_user_message(self, content: str) -> None:
        self.messages.append(Message(role="user", content=content))
        self._trim()
        
    def add_assistant_message(self, content: str) -> None:
        self.messages.append(Message(role="assistant", content=content))
        self._trim()
    
    def _trim(self) -> None:
        while len(self.messages) > self.max_messages:
            self.messages.pop(0)
    
    def get_context_summary(self) -> str:
        if not self.messages:
            return ""
        lines = ["=== å°è©±æ­·å²è„ˆçµ¡ ==="]
        for i, msg in enumerate(self.messages):
            prefix = "User" if msg.role == "user" else "Claude"
            preview = msg.content[:500] + "..." if len(msg.content) > 500 else msg.content
            lines.append(f"[{i+1}] {prefix}: {preview}")
        lines.append("=== ç•¶å‰æŒ‡ä»¤ ===")
        return "\n".join(lines)
    
    def clear(self) -> None:
        self.messages.clear()
        logger.info("å°è©±æ­·å²å·²æ¸…ç©º")
    
    def save(self, filepath: Path) -> None:
        try:
            filepath.parent.mkdir(parents=True, exist_ok=True)
            data = {"messages": [asdict(m) for m in self.messages], "saved_at": datetime.now().isoformat()}
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ­·å²å¤±æ•—: {e}")
    
    @classmethod
    def load(cls, filepath: Path, max_rounds: int = 10):
        history = cls(max_rounds=max_rounds)
        try:
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                for msg_data in data.get("messages", []):
                    history.messages.append(Message(**msg_data))
                logger.info(f"å·²è¼‰å…¥ {len(history.messages)} æ¢æ­·å²è¨Šæ¯")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ­·å²å¤±æ•—: {e}")
        return history


# === ä¸»æ©‹æ¥å™¨ ===

class ClaudeBridge:
    def __init__(self):
        self.history = ConversationHistory.load(CONFIG["HISTORY_FILE"], CONFIG["MAX_HISTORY_ROUNDS"])
        self.is_busy = False
        self.special_commands = {
            "/clear": self._cmd_clear,
            "/history": self._cmd_show_history,
            "/help": self._cmd_help,
            "/status": self._cmd_status,
            "/extract": self._cmd_extract,
            "/fetch": self._cmd_fetch,
        }
    
    def is_authorized(self, user_id: int) -> bool:
        if not CONFIG["ALLOWED_USER_IDS"]:
            return True
        return user_id in CONFIG["ALLOWED_USER_IDS"]
    
    async def _cmd_clear(self, chat_id: int) -> str:
        self.history.clear()
        self.history.save(CONFIG["HISTORY_FILE"])
        return "å°è©±æ­·å²å·²æ¸…ç©ºã€‚æ–°çš„å°è©±å°‡ä¸æœƒåŒ…å«ä¹‹å‰çš„è„ˆçµ¡ã€‚"
    
    async def _cmd_show_history(self, chat_id: int) -> str:
        if not self.history.messages:
            return "ç›®å‰æ²’æœ‰å°è©±æ­·å²ã€‚"
        lines = [f"å°è©±æ­·å² ({len(self.history.messages)} æ¢):"]
        for i, msg in enumerate(self.history.messages):
            prefix = "User" if msg.role == "user" else "Claude"
            preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            preview = preview.replace('\n', ' ')
            lines.append(f"[{i+1}] {prefix}: {preview}")
        return "\n".join(lines)
    
    async def _cmd_help(self, chat_id: int) -> str:
        # å‹•æ…‹é¡¯ç¤ºå¯ç”¨åŠŸèƒ½
        url_status = []
        url_status.append(f"  fxtwitter (X/Twitter): {'âœ… å¯ç”¨' if REQUESTS_AVAILABLE else 'âŒ éœ€è¦ requests'}")
        url_status.append(f"  yt-dlp (YouTube/é€šç”¨): {'âœ… å¯ç”¨' if YTDLP_AVAILABLE else 'âŒ æœªå®‰è£'}")
        url_status.append(f"  HTTP fallback: {'âœ… å¯ç”¨' if REQUESTS_AVAILABLE else 'âŒ éœ€è¦ requests'}")
        url_block = "\n".join(url_status)
        
        return f"""Telegram Claude Bridge v2.4 æŒ‡ä»¤èªªæ˜

ç‰¹æ®ŠæŒ‡ä»¤ï¼š
/clear - æ¸…ç©ºå°è©±æ­·å²
/history - é¡¯ç¤ºç›®å‰çš„å°è©±æ­·å²æ‘˜è¦
/status - é¡¯ç¤ºç³»çµ±ç‹€æ…‹
/help - é¡¯ç¤ºæ­¤å¹«åŠ©è¨Šæ¯
/exec <cmd> - ç›´æ¥åŸ·è¡Œ PowerShell å‘½ä»¤

ä¸€èˆ¬ä½¿ç”¨ï¼š
ç›´æ¥è¼¸å…¥è¨Šæ¯å³å¯èˆ‡ Claude Code å°è©±ã€‚
ç³»çµ±æœƒè‡ªå‹•ä¿ç•™æœ€è¿‘ {CONFIG['MAX_HISTORY_ROUNDS']} è¼ªå°è©±ä½œç‚ºä¸Šä¸‹æ–‡ã€‚

ğŸ”— URL è‡ªå‹•è™•ç†ï¼ˆv2.4 æ–°å¢ï¼‰ï¼š
åˆ†äº«ä»»ä½•é€£çµï¼Œç³»çµ±æœƒè‡ªå‹•æŠ“å–å…§å®¹ä¸¦æä¾›çµ¦ Claudeï¼š
- X/Twitter â†’ fxtwitter APIï¼ˆå¿«é€Ÿï¼‰â†’ yt-dlpï¼ˆå‚™ç”¨ï¼‰
- YouTube â†’ yt-dlp
- å…¶ä»–ç¶²ç«™ â†’ HTTP æŠ“å–æ¨™é¡Œ/æè¿°

URL è™•ç†å™¨ç‹€æ…‹ï¼š
{url_block}

Log ç®¡ç†ï¼š
- æ¯æ—¥ç”¢ç”Ÿç¨ç«‹ log æª”æ¡ˆ
- è‡ªå‹•æ¸…ç† {CONFIG['LOG_RETENTION_DAYS']} å¤©å‰çš„èˆŠ log
"""
    
    async def _cmd_fetch(self, chat_id: int) -> str:
        # Fetch URL, analyze, save AI-friendly output
        if not self.history.messages:
            return "No messages. Usage: /fetch <URL> [notes]"
        last_user = None
        for msg in reversed(self.history.messages):
            if msg.role == "user":
                last_user = msg.content
                break
        if not last_user:
            return "No user message found."
        urls = detect_urls(last_user)
        if not urls:
            return "No URL found in last message."
        url = urls[0][0]
        user_note = last_user.replace(url, "").replace("/fetch", "").strip()
        enhanced_text, summaries = await preprocess_urls(url)
        fetched = enhanced_text if enhanced_text != url else "Could not fetch"
        fetch_prompt = "URL content:" + chr(10) + fetched + chr(10) + chr(10)
        if user_note:
            fetch_prompt += "User task: " + user_note + chr(10) + chr(10)
        fetch_prompt += "Provide comprehensive analysis in Traditional Chinese. "
        fetch_prompt += "Structure clearly. This will be shared with other AI models."
        response = await self.execute_claude(fetch_prompt)
        saved = await asyncio.get_event_loop().run_in_executor(
            None, save_fetch_output, url, fetched, response, user_note
        )
        if saved:
            return response + chr(10) + chr(10) + "---" + chr(10) + "Saved: " + saved
        return response
    
    async def _cmd_extract(self, chat_id: int) -> str:
        # Structured extraction on last assistant message
        if not self.history.messages:
            return "No history to extract."
        last_assistant = None
        for msg in reversed(self.history.messages):
            if msg.role == "assistant":
                last_assistant = msg.content
                break
        if not last_assistant:
            return "No assistant reply found."
        result = await asyncio.get_event_loop().run_in_executor(None, extract_structured_data, last_assistant)
        return result or "No extraction result"
    
    async def _cmd_status(self, chat_id: int) -> str:
        log_files = list(CONFIG["LOG_DIR"].glob("bridge.log*"))
        status = "å¿™ç¢Œä¸­" if self.is_busy else "å¾…å‘½"
        return f"""ç³»çµ±ç‹€æ…‹ (v2.4)
æ­·å²è¨Šæ¯æ•¸: {len(self.history.messages)}
æ­·å²æª”æ¡ˆ: {CONFIG['HISTORY_FILE']}
æœ€å¤§ä¿ç•™è¼ªæ•¸: {CONFIG['MAX_HISTORY_ROUNDS']}
å·¥ä½œç›®éŒ„: {CONFIG['WORKING_DIR']}
Log ç›®éŒ„: {CONFIG['LOG_DIR']}
Log æª”æ¡ˆæ•¸: {len(log_files)}
Log ä¿ç•™å¤©æ•¸: {CONFIG['LOG_RETENTION_DAYS']}
Claude ç‹€æ…‹: {status}

URL è™•ç†å™¨:
  fxtwitter: {'âœ…' if REQUESTS_AVAILABLE else 'âŒ'}
  yt-dlp: {'âœ…' if YTDLP_AVAILABLE else 'âŒ'}
  HTTP fallback: {'âœ…' if REQUESTS_AVAILABLE else 'âŒ'}

ç•¶å‰æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

    def _build_prompt_with_context(self, user_message: str) -> str:
        context = self.history.get_context_summary()
        safety_note = "å®‰å…¨é™åˆ¶ï¼šä¸è¦åˆªé™¤é‡è¦æª”æ¡ˆï¼Œä¸è¦ä¿®æ”¹ç³»çµ±è¨­å®šã€‚" if not CONFIG["ALLOW_DANGEROUS"] else ""
        if context:
            return f"{context}\n{user_message}\n\n{safety_note}\nè«‹åŸºæ–¼ä¸Šè¿°å°è©±è„ˆçµ¡ä¾†ç†è§£å’ŒåŸ·è¡Œç•¶å‰æŒ‡ä»¤ã€‚å›æ‡‰è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
        return f"{user_message}\n\n{safety_note}\nå›æ‡‰è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"

    async def execute_claude(self, prompt: str) -> str:
        if self.is_busy:
            return "Claude æ­£åœ¨è™•ç†å¦ä¸€å€‹ä»»å‹™ï¼Œè«‹ç¨å¾Œå†è©¦..."
        self.is_busy = True
        try:
            full_prompt = self._build_prompt_with_context(prompt)
            logger.info(f"åŸ·è¡Œ Claude æŒ‡ä»¤ï¼š{prompt[:100]}...")
            process = await asyncio.create_subprocess_shell(
                f'"{CONFIG["CLAUDE_CLI"]}" --print --dangerously-skip-permissions',
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(CONFIG["WORKING_DIR"])
            )
            stdout, stderr = await asyncio.wait_for(
                process.communicate(input=full_prompt.encode('utf-8')),
                timeout=CONFIG["TIMEOUT"]
            )
            output = stdout.decode('utf-8', errors='replace')
            error = stderr.decode('utf-8', errors='replace')
            if error and not output:
                result = f"éŒ¯èª¤ï¼š\n{error}"
            elif output:
                result = self._format_output(output)
            else:
                result = "ä»»å‹™å®Œæˆï¼ˆç„¡è¼¸å‡ºï¼‰"
            return result
        except asyncio.TimeoutError:
            return f"åŸ·è¡Œè¶…æ™‚ï¼ˆ{CONFIG['TIMEOUT']}ç§’ï¼‰"
        except FileNotFoundError:
            return "æ‰¾ä¸åˆ° Claude CLIã€‚è«‹ç¢ºä¿å·²å®‰è£ Claude Codeã€‚"
        except Exception as e:
            logger.error(f"Claude åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")
            return f"åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}"
        finally:
            self.is_busy = False
    
    def _format_output(self, output: str) -> str:
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        output = ansi_escape.sub('', output)
        if len(output) > 3500:
            output = output[:3500] + "\n\n...(è¼¸å‡ºå·²æˆªæ–·)"
        return output
    
    async def handle_message(self, chat_id: int, text: str) -> Tuple[str, Optional[str]]:
        """
        è™•ç†è¨Šæ¯ã€‚å›å‚³ (claude_response, url_status_msg)ã€‚
        url_status_msg ç‚º None è¡¨ç¤ºæ²’æœ‰ URL è™•ç†ã€‚
        """
        text = text.strip()
        cmd = text.split()[0].lower() if text else ""
        if cmd in self.special_commands:
            return await self.special_commands[cmd](chat_id), None
        
        logger.info(f"æ”¶åˆ°è¨Šæ¯ (chat_id={chat_id}): {text[:100]}...")
        
        # === URL é è™•ç† ===
        enhanced_text, url_summaries = await preprocess_urls(text)
        
        url_status = None
        if url_summaries:
            url_status = "ğŸ”— URL è™•ç†çµæœ:\n" + "\n".join(url_summaries)
            logger.info(f"URL é è™•ç†å®Œæˆ: {url_summaries}")
        
        # è¨˜éŒ„åŸå§‹è¨Šæ¯ï¼ˆä¸å«æŠ“å–å…§å®¹ï¼Œé¿å…æ­·å²éé•·ï¼‰
        self.history.add_user_message(text)
        
        # å‚³é€å¢å¼·å¾Œçš„è¨Šæ¯çµ¦ Claude
        response = await self.execute_claude(enhanced_text)
        
        # Auto-save fetch output when URLs present
        if url_summaries:
            detected = detect_urls(text)
            if detected:
                fetch_url = detected[0][0]
                user_note = text.replace(fetch_url, "").strip()
                await asyncio.get_event_loop().run_in_executor(None, save_fetch_output, fetch_url, enhanced_text, response, user_note)
        self.history.add_assistant_message(response)
        self.history.save(CONFIG["HISTORY_FILE"])
        
        return response, url_status


# === Telegram Handlers ===

bridge = None

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    if not bridge.is_authorized(user.id):
        await update.message.reply_text(f"æœªæˆæ¬Šçš„ç”¨æˆ¶\nä½ çš„ User ID: {user.id}")
        return
    
    url_features = []
    if REQUESTS_AVAILABLE:
        url_features.append("fxtwitter (X/Twitter)")
    if YTDLP_AVAILABLE:
        url_features.append("yt-dlp (YouTube/é€šç”¨)")
    url_text = "ã€".join(url_features) if url_features else "æœªå•Ÿç”¨"
    
    await update.message.reply_text(
        f"Telegram Claude Code æ©‹æ¥å™¨ v2.4\n\n"
        f"æ­¡è¿ï¼Œ{user.first_name}ï¼\n\n"
        f"åŠŸèƒ½ï¼š\n"
        f"- è‡ªå‹•ä¿ç•™æœ€è¿‘ {CONFIG['MAX_HISTORY_ROUNDS']} è¼ªå°è©±\n"
        f"- æ¯æ—¥ç¨ç«‹ logï¼Œè‡ªå‹•æ¸…ç† {CONFIG['LOG_RETENTION_DAYS']} å¤©å‰çš„èˆŠæª”\n"
        f"- ğŸ†• URL è‡ªå‹•æŠ“å–: {url_text}\n\n"
        f"è¼¸å…¥ /help æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤ã€‚"
    )

async def exec_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not bridge.is_authorized(update.effective_user.id):
        await update.message.reply_text("æœªæˆæ¬Š")
        return
    if not context.args:
        await update.message.reply_text("ç”¨æ³•ï¼š/exec <PowerShellå‘½ä»¤>")
        return
    command = ' '.join(context.args)
    await update.message.reply_text(f"åŸ·è¡Œä¸­ï¼š{command}")
    try:
        result = subprocess.run(
            ["powershell", "-Command", command],
            capture_output=True, text=True, timeout=60,
            cwd=str(CONFIG["WORKING_DIR"]), encoding='utf-8', errors='replace'
        )
        output = result.stdout or result.stderr or "(ç„¡è¼¸å‡º)"
        if len(output) > 3500:
            output = output[:3500] + "\n...(å·²æˆªæ–·)"
        status = "æˆåŠŸ" if result.returncode == 0 else "å¤±æ•—"
        await update.message.reply_text(f"{status} çµæœï¼š\n{output}")
    except subprocess.TimeoutExpired:
        await update.message.reply_text("åŸ·è¡Œè¶…æ™‚")
    except Exception as e:
        await update.message.reply_text(f"éŒ¯èª¤ï¼š{e}")

async def message_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if not bridge.is_authorized(user_id):
        await update.message.reply_text(f"æœªæˆæ¬Š\nä½ çš„ User ID: {user_id}")
        return
    
    text = update.message.text
    
    # åµæ¸¬æ˜¯å¦åŒ…å« URLï¼Œçµ¦ä¸åŒçš„ç­‰å¾…è¨Šæ¯
    urls = detect_urls(text)
    if urls:
        url_list = ", ".join([u[0][:40] + "..." if len(u[0]) > 40 else u[0] for u, _ in [urls[0]]])
        processing_msg = await update.message.reply_text(
            f"ğŸ”— åµæ¸¬åˆ°é€£çµï¼Œæ­£åœ¨æŠ“å–å…§å®¹...\n{text[:50]}{'...' if len(text) > 50 else ''}"
        )
    else:
        processing_msg = await update.message.reply_text(
            f"Claude æ­£åœ¨è™•ç†...\n{text[:50]}{'...' if len(text) > 50 else ''}"
        )
    
    result, url_status = await bridge.handle_message(update.effective_chat.id, text)
    
    try:
        await processing_msg.delete()
    except:
        pass
    
    # å¦‚æœæœ‰ URL è™•ç†çµæœï¼Œå…ˆç™¼ä¸€æ¢ç‹€æ…‹è¨Šæ¯
    if url_status:
        await update.message.reply_text(url_status)
    
    # ç™¼é€ Claude å›æ‡‰
    if len(result) > 4000:
        chunks = [result[i:i+4000] for i in range(0, len(result), 4000)]
        for i, chunk in enumerate(chunks):
            await update.message.reply_text(f"[{i+1}/{len(chunks)}]\n\n{chunk}")
    else:
        await update.message.reply_text(f"Claude å›æ‡‰ï¼š\n\n{result}")

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{context.error}")
    if update and update.message:
        await update.message.reply_text("ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æŸ¥çœ‹æ—¥èªŒ")

def find_claude_cli():
    paths = [
        CONFIG["CLAUDE_CLI"], "claude",
        os.path.expandvars(r"%APPDATA%\npm\claude.cmd"),
        r"C:\Users\USER\AppData\Roaming\npm\claude.cmd"
    ]
    for path in paths:
        try:
            result = subprocess.run([path, "--version"], capture_output=True, text=True, timeout=10, shell=True)
            if result.returncode == 0:
                logger.info(f"æ‰¾åˆ° Claude CLIï¼š{path}")
                return path
        except:
            continue
    return None

def main():
    global bridge
    
    logger.info("=" * 50)
    logger.info("å•Ÿå‹• Telegram Claude Code æ©‹æ¥å™¨ v2.4")
    logger.info("=" * 50)
    
    cleanup_old_logs()
    
    if not TELEGRAM_LIB_AVAILABLE:
        print("éŒ¯èª¤: è«‹å…ˆå®‰è£ python-telegram-bot")
        sys.exit(1)
    
    claude_path = find_claude_cli()
    if claude_path:
        CONFIG["CLAUDE_CLI"] = claude_path
    else:
        logger.error("æ‰¾ä¸åˆ° Claude CLIï¼")
        return
    
    bridge = ClaudeBridge()
    
    application = Application.builder().token(CONFIG["TELEGRAM_BOT_TOKEN"]).build()
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("exec", exec_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, message_handler))
    application.add_error_handler(error_handler)
    
    logger.info(f"æ­·å²è¨Šæ¯æ•¸ï¼š{len(bridge.history.messages)}")
    logger.info(f"æœ€å¤§ä¿ç•™è¼ªæ•¸ï¼š{CONFIG['MAX_HISTORY_ROUNDS']}")
    logger.info(f"Log ç›®éŒ„ï¼š{CONFIG['LOG_DIR']}")
    logger.info(f"Log ä¿ç•™å¤©æ•¸ï¼š{CONFIG['LOG_RETENTION_DAYS']}")
    logger.info(f"URL è™•ç†å™¨: fxtwitter={'âœ…' if REQUESTS_AVAILABLE else 'âŒ'}, yt-dlp={'âœ…' if YTDLP_AVAILABLE else 'âŒ'}")
    logger.info("Bot å·²å•Ÿå‹•ï¼Œç­‰å¾… Telegram è¨Šæ¯...")
    
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
