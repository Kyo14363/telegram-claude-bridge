"""
url_fetchers.py â€” URL æŠ“å–èˆ‡å¹³å°è§£ææ¨¡çµ„
==========================================
åŒ…å« URL åµæ¸¬ã€å¹³å°ç‰¹å®š fetcherã€LangExtract å¢å¼·ã€
ä»¥åŠ preprocess_urls() ç·¨æ’å‡½å¼ã€‚
"""

import os
import re
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Tuple

logger = logging.getLogger(__name__)

# --- å¯ç”¨æ€§æª¢æ¸¬ ---

try:
    import yt_dlp
    YTDLP_AVAILABLE = True
    logger.info("yt-dlp å¯ç”¨ï¼Œå·²å•Ÿç”¨ä½œç‚ºå‚™ç”¨ URL è™•ç†å™¨")
except ImportError:
    YTDLP_AVAILABLE = False
    logger.info("yt-dlp æœªå®‰è£ï¼Œåƒ…ä½¿ç”¨ fxtwitter/HTTP æ–¹æ¡ˆè™•ç† URL")

try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    LANGEXTRACT_AVAILABLE = False
    lx = None

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    logger.warning("requests æœªå®‰è£ï¼ŒURL é è™•ç†åŠŸèƒ½å°‡å—é™")

# vision æ¨¡çµ„ â€” å»¶é² import é¿å…å¾ªç’°ä¾è³´
from vision import analyze_images, GENAI_AVAILABLE


# --- URL åµæ¸¬èˆ‡åˆ†é¡ ---

PLATFORM_PATTERNS = {
    "x_twitter": [
        r"(?:https?://)?(?:www\.)?(?:twitter\.com|x\.com)/\S+",
        r"(?:https?://)?t\.co/\S+",
    ],
    "youtube": [
        r"(?:https?://)?(?:www\.)?youtube\.com/watch\S+",
        r"(?:https?://)?youtu\.be/\S+",
        r"(?:https?://)?(?:www\.)?youtube\.com/shorts/\S+",
    ],
    "general": [
        r"https?://\S+",
    ],
}


def detect_urls(text: str) -> List[Tuple[str, str]]:
    """
    å¾è¨Šæ¯ä¸­åµæ¸¬ URL ä¸¦åˆ†é¡å¹³å°ã€‚
    å›å‚³ [(url, platform), ...] çš„åˆ—è¡¨ã€‚
    å„ªå…ˆåŒ¹é…ç‰¹å®šå¹³å°ï¼Œæœ€å¾Œæ‰åŒ¹é… generalã€‚
    """
    found = []
    found_urls = set()

    for platform in ["x_twitter", "youtube"]:
        for pattern in PLATFORM_PATTERNS[platform]:
            for match in re.finditer(pattern, text):
                url = match.group(0)
                if url not in found_urls:
                    found_urls.add(url)
                    found.append((url, platform))

    for pattern in PLATFORM_PATTERNS["general"]:
        for match in re.finditer(pattern, text):
            url = match.group(0)
            if url not in found_urls:
                found_urls.add(url)
                found.append((url, "general"))

    return found


# --- æ–¹æ¡ˆ D: fxtwitter (X/Twitter å°ˆç”¨) ---

def fetch_via_fxtwitter(url: str, config: dict = None) -> Optional[Tuple[str, List[str]]]:
    """
    ç”¨ fxtwitter.com API æŠ“å– X/Twitter æ¨æ–‡å…§å®¹ã€‚
    å°‡ x.com / twitter.com æ›¿æ›æˆ api.fxtwitter.com å–å¾— JSONã€‚
    å›å‚³ (text_content, image_urls) tupleï¼Œæˆ– Noneã€‚
    """
    if not REQUESTS_AVAILABLE:
        return None

    cfg = config or {}
    fetch_timeout = cfg.get("URL_FETCH_TIMEOUT", 15)
    max_images = cfg.get("MAX_IMAGES_PER_MESSAGE", 5)

    try:
        api_url = re.sub(
            r"https?://(www\.)?(twitter\.com|x\.com)",
            "https://api.fxtwitter.com",
            url
        )

        logger.info(f"[fxtwitter] å˜—è©¦æŠ“å–: {api_url}")

        resp = requests.get(api_url, timeout=fetch_timeout, headers={
            "User-Agent": "TelegramClaudeBridge/2.6"
        })

        if resp.status_code != 200:
            logger.warning(f"[fxtwitter] HTTP {resp.status_code}")
            return None

        data = resp.json()
        tweet = data.get("tweet", {})

        if not tweet:
            logger.warning("[fxtwitter] å›æ‡‰ä¸­ç„¡ tweet è³‡æ–™")
            return None

        parts = []
        parts.append(f"ğŸ“Œ æ¨æ–‡ä¾†æº: {url}")

        author = tweet.get("author", {})
        if author:
            parts.append(f"ğŸ‘¤ ä½œè€…: {author.get('name', '?')} (@{author.get('screen_name', '?')})")

        text = tweet.get("text", "")
        if text:
            parts.append(f"ğŸ“ å…§å®¹:\n{text}")

        # Twitter Articleï¼ˆé•·æ–‡ / Notesï¼‰
        article = tweet.get("article")
        if article:
            article_title = article.get("title", "")
            if article_title:
                parts.append(f"ğŸ“° é•·æ–‡æ¨™é¡Œ: {article_title}")
            # è§£æ article content blocks
            content_blocks = article.get("content", {}).get("blocks", [])
            if content_blocks:
                article_texts = []
                for block in content_blocks:
                    block_text = block.get("text", "").strip()
                    if block_text:
                        block_type = block.get("type", "unstyled")
                        if block_type.startswith("header"):
                            article_texts.append(f"\n## {block_text}")
                        elif block_type == "blockquote":
                            article_texts.append(f"> {block_text}")
                        elif block_type in ("ordered-list-item", "unordered-list-item"):
                            article_texts.append(f"- {block_text}")
                        else:
                            article_texts.append(block_text)
                if article_texts:
                    article_body = "\n".join(article_texts)
                    parts.append(f"ğŸ“ é•·æ–‡å…§å®¹:\n{article_body}")
                    logger.info(f"[fxtwitter] è§£æåˆ° Articleï¼Œ{len(content_blocks)} å€‹ blocksï¼Œ{len(article_body)} å­—å…ƒ")

        # åª’é«”è³‡è¨Š
        media = tweet.get("media", {})
        image_urls = []
        if media:
            photos = media.get("photos", [])
            videos = media.get("videos", [])
            if photos:
                for photo in photos[:max_images]:
                    photo_url = photo.get("url")
                    if photo_url:
                        image_urls.append(photo_url)
                parts.append(f"ğŸ–¼ï¸ åŒ…å« {len(photos)} å¼µåœ–ç‰‡")
            if videos:
                # Twitter GIF è¢«æ­¸é¡ç‚º videoï¼ˆtype="gif"ï¼‰ï¼Œå–å…¶ thumbnail é€²åœ–ç‰‡åˆ†æ
                gif_count = 0
                for video in videos:
                    if video.get("type") == "gif" and video.get("thumbnail_url"):
                        if len(image_urls) < max_images:
                            image_urls.append(video["thumbnail_url"])
                            gif_count += 1
                if gif_count:
                    parts.append(f"ğŸï¸ åŒ…å« {gif_count} å€‹ GIFï¼ˆå·²æ“·å–ç¸®åœ–ä¾›åˆ†æï¼‰")
                real_video_count = len(videos) - gif_count
                if real_video_count > 0:
                    parts.append(f"ğŸ¬ åŒ…å« {real_video_count} å€‹å½±ç‰‡")

        # äº’å‹•æ•¸æ“š
        likes = tweet.get("likes", 0)
        retweets = tweet.get("retweets", 0)
        replies = tweet.get("replies", 0)
        if likes or retweets or replies:
            parts.append(f"ğŸ’¬ äº’å‹•: {likes} è®š / {retweets} è½‰æ¨ / {replies} å›è¦†")

        created = tweet.get("created_at", "")
        if created:
            parts.append(f"ğŸ“… ç™¼å¸ƒæ™‚é–“: {created}")

        # å¼•ç”¨æ¨æ–‡
        quote = tweet.get("quote", {})
        if quote:
            quote_author = quote.get("author", {})
            quote_text = quote.get("text", "")
            parts.append(f"\nâ†©ï¸ å¼•ç”¨æ¨æ–‡ (@{quote_author.get('screen_name', '?')}):\n{quote_text}")

        result = "\n".join(parts)
        logger.info(f"[fxtwitter] æˆåŠŸæŠ“å–æ¨æ–‡ï¼Œ{len(result)} å­—å…ƒï¼Œ{len(image_urls)} å¼µåœ–ç‰‡ URL")
        return result, image_urls

    except requests.Timeout:
        logger.warning(f"[fxtwitter] è«‹æ±‚è¶…æ™‚")
        return None
    except Exception as e:
        logger.error(f"[fxtwitter] éŒ¯èª¤: {e}")
        return None


# --- æ–¹æ¡ˆ C: yt-dlp (é€šç”¨å‚™ç”¨) ---

def fetch_via_ytdlp(url: str, config: dict = None) -> Optional[str]:
    """
    ç”¨ yt-dlp æå– URL çš„ metadataï¼ˆä¸ä¸‹è¼‰æª”æ¡ˆï¼‰ã€‚
    æ”¯æ´ X/Twitterã€YouTubeã€TikTok ç­‰ä¸Šåƒå€‹å¹³å°ã€‚
    """
    if not YTDLP_AVAILABLE:
        return None

    cfg = config or {}
    fetch_timeout = cfg.get("URL_FETCH_TIMEOUT", 15)

    try:
        logger.info(f"[yt-dlp] å˜—è©¦æŠ“å–: {url}")

        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'skip_download': True,
            'socket_timeout': fetch_timeout,
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)

        if not info:
            return None

        parts = []
        parts.append(f"ğŸ”— ä¾†æº: {url}")

        title = info.get('title')
        if title:
            parts.append(f"ğŸ“Œ æ¨™é¡Œ: {title}")

        uploader = info.get('uploader') or info.get('channel')
        if uploader:
            parts.append(f"ğŸ‘¤ ä½œè€…/é »é“: {uploader}")

        description = info.get('description', '')
        if description:
            desc_preview = description[:1000]
            if len(description) > 1000:
                desc_preview += "...(å·²æˆªæ–·)"
            parts.append(f"ğŸ“ æè¿°/å…§å®¹:\n{desc_preview}")

        duration = info.get('duration')
        if duration:
            mins, secs = divmod(int(duration), 60)
            hours, mins = divmod(mins, 60)
            if hours:
                parts.append(f"â±ï¸ æ™‚é•·: {hours}:{mins:02d}:{secs:02d}")
            else:
                parts.append(f"â±ï¸ æ™‚é•·: {mins}:{secs:02d}")

        view_count = info.get('view_count')
        like_count = info.get('like_count')
        if view_count or like_count:
            stats = []
            if view_count:
                stats.append(f"{view_count:,} è§€çœ‹")
            if like_count:
                stats.append(f"{like_count:,} è®š")
            parts.append(f"ğŸ“Š æ•¸æ“š: {' / '.join(stats)}")

        upload_date = info.get('upload_date')
        if upload_date:
            try:
                formatted = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:]}"
                parts.append(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸ: {formatted}")
            except:
                pass

        subtitles = info.get('subtitles', {})
        auto_subs = info.get('automatic_captions', {})
        if subtitles or auto_subs:
            langs = list(subtitles.keys()) + list(auto_subs.keys())
            parts.append(f"ğŸ’¬ å¯ç”¨å­—å¹•èªè¨€: {', '.join(langs[:10])}")

        result = "\n".join(parts)
        logger.info(f"[yt-dlp] æˆåŠŸæŠ“å– metadataï¼Œ{len(result)} å­—å…ƒ")
        return result

    except Exception as e:
        logger.error(f"[yt-dlp] éŒ¯èª¤: {e}")
        return None


# --- æ–¹æ¡ˆ fallback: åŸºæœ¬ HTTP æŠ“å– ---

def fetch_via_http(url: str, config: dict = None) -> Optional[str]:
    """
    åŸºæœ¬ HTTP GETï¼Œå˜—è©¦æŠ“å–é é¢æ¨™é¡Œå’Œ meta descriptionã€‚
    ä½œç‚ºæœ€å¾Œçš„ fallbackã€‚
    """
    if not REQUESTS_AVAILABLE:
        return None

    cfg = config or {}
    fetch_timeout = cfg.get("URL_FETCH_TIMEOUT", 15)

    try:
        logger.info(f"[http] å˜—è©¦æŠ“å–: {url}")

        resp = requests.get(url, timeout=fetch_timeout, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }, allow_redirects=True)

        if resp.status_code != 200:
            return None

        content = resp.text[:10000]

        parts = [f"ğŸ”— ä¾†æº: {url}"]

        title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
        if title_match:
            title = re.sub(r'\s+', ' ', title_match.group(1)).strip()
            parts.append(f"ğŸ“Œ æ¨™é¡Œ: {title}")

        og_title = re.search(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)
        og_desc = re.search(r'<meta[^>]*property=["\']og:description["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)
        meta_desc = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\'](.*?)["\']', content, re.IGNORECASE)

        if og_title:
            parts.append(f"ğŸ“Œ OG æ¨™é¡Œ: {og_title.group(1)}")

        desc = (og_desc and og_desc.group(1)) or (meta_desc and meta_desc.group(1))
        if desc:
            parts.append(f"ğŸ“ æè¿°: {desc}")

        if len(parts) <= 1:
            return None

        result = "\n".join(parts)
        logger.info(f"[http] æˆåŠŸæŠ“å–åŸºæœ¬è³‡è¨Šï¼Œ{len(result)} å­—å…ƒ")
        return result

    except Exception as e:
        logger.error(f"[http] éŒ¯èª¤: {e}")
        return None


# --- LangExtract ---

def enhance_with_langextract(raw_content, url):
    """Use LangExtract to extract structured info from fetched web content."""
    if not LANGEXTRACT_AVAILABLE or len(raw_content) < 200:
        return None
    try:
        if not os.getenv('GOOGLE_API_KEY'):
            return None
        logger.info(f'[langextract] extracting ({len(raw_content)} chars)...')
        extract_results = lx.extract(
            text=raw_content[:5000],
            prompt='Extract key information: main topic, key claims/data, people/orgs, numbers/stats, conclusion.',
            model='gemini-2.0-flash'
        )
        if not extract_results:
            return None
        result_text = str(extract_results)
        if len(result_text) < 50:
            return None
        sep = chr(10) + chr(10)
        return raw_content + sep + '=== LangExtract ===' + chr(10) + result_text[:2000] + chr(10) + '=== end ==='
    except Exception as e:
        logger.error(f'[langextract] failed: {e}')
        return None


def extract_structured_data(text, prompt=None):
    """/extract command: structured extraction on any text."""
    if not LANGEXTRACT_AVAILABLE:
        return 'LangExtract not installed'
    if not os.getenv('GOOGLE_API_KEY'):
        return 'GOOGLE_API_KEY not set'
    try:
        dp = 'Extract all key entities, facts, numbers, relationships. Organize in structured format.'
        res = lx.extract(text=text[:8000], prompt=prompt or dp, model='gemini-2.0-flash')
        if res:
            return 'LangExtract result:' + chr(10) + chr(10) + str(res)[:3000]
        return 'Extraction complete but no results'
    except Exception as e:
        return f'Extraction failed: {e}'


# --- Fetch Output å„²å­˜ ---

def save_fetch_output(url, fetched_content, claude_response, user_note="", config: dict = None):
    """Save AI-friendly markdown summary to fetch_outputs/."""
    cfg = config or {}
    output_dir = cfg.get("FETCH_OUTPUT_DIR", Path(r"C:\telegram-MCP-bridge\fetch_outputs"))

    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_url = re.sub(r"[^a-zA-Z0-9]", "_", url[:60])
        filename = f"fetch_{ts}_{safe_url}.md"
        filepath = output_dir / filename
        sep = chr(10)
        parts = []
        parts.append("# AI-Friendly Content Summary")
        parts.append("")
        parts.append(f"- **Source**: {url}")
        parts.append(f"- **Fetched**: {datetime.now().isoformat()}")
        if user_note:
            parts.append(f"- **User Note**: {user_note}")
        parts.append("")
        parts.append("---")
        parts.append("")
        parts.append("## Fetched Content")
        parts.append("")
        parts.append(fetched_content)
        parts.append("")
        parts.append("---")
        parts.append("")
        parts.append("## Claude Analysis")
        parts.append("")
        parts.append(claude_response)
        content = sep.join(parts)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        logger.info(f"[fetch] Saved: {filepath} ({len(content)} chars)")
        return str(filepath)
    except Exception as e:
        logger.error(f"[fetch] Save failed: {e}")
        return None


# --- URL é è™•ç†ç·¨æ’å™¨ ---

async def preprocess_urls(text: str, config: dict = None) -> Tuple[str, List[str]]:
    """
    åµæ¸¬è¨Šæ¯ä¸­çš„ URLï¼Œè‡ªå‹•æŠ“å–å…§å®¹ï¼Œå›å‚³å¢å¼·å¾Œçš„è¨Šæ¯ã€‚

    ç­–ç•¥ï¼š
    - X/Twitter: fxtwitter (æ–¹æ¡ˆD) â†’ yt-dlp (æ–¹æ¡ˆC) â†’ http fallback
    - YouTube/å…¶ä»– yt-dlp æ”¯æ´å¹³å°: yt-dlp (æ–¹æ¡ˆC) â†’ http fallback
    - å…¶ä»– URL: http fallback

    å›å‚³: (å¢å¼·å¾Œçš„å®Œæ•´è¨Šæ¯, è™•ç†æ‘˜è¦åˆ—è¡¨)
    """
    cfg = config or {}
    urls = detect_urls(text)

    if not urls:
        return text, []

    logger.info(f"åµæ¸¬åˆ° {len(urls)} å€‹ URL: {urls}")

    enrichments = []
    summaries = []

    for url, platform in urls:
        content = None
        method_used = None

        if platform == "x_twitter":
            # X/Twitter: fxtwitter (å›å‚³ tuple) â†’ yt-dlp â†’ http
            fxt_result = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_fxtwitter, url, cfg
            )
            if fxt_result is not None:
                content, image_urls = fxt_result
                method_used = "fxtwitter"

                # å±¤æ¬¡äºŒï¼šé€šç”¨åœ–ç‰‡åˆ†æ
                if image_urls:
                    tweet_text = ""
                    for line in content.split("\n"):
                        if line.startswith("ğŸ“ å…§å®¹:"):
                            tweet_text = line.replace("ğŸ“ å…§å®¹:", "").strip()
                            break
                    image_descriptions = await asyncio.get_event_loop().run_in_executor(
                        None, analyze_images, image_urls, tweet_text, cfg
                    )
                    if image_descriptions:
                        content = content + "\n\n" + image_descriptions
                        method_used = "fxtwitter+img"
            else:
                content = await asyncio.get_event_loop().run_in_executor(
                    None, fetch_via_ytdlp, url, cfg
                )
                if content:
                    method_used = "yt-dlp"

        elif platform == "youtube":
            content = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_ytdlp, url, cfg
            )
            if content:
                method_used = "yt-dlp"

        # é€šç”¨ fallback
        if not content:
            content = await asyncio.get_event_loop().run_in_executor(
                None, fetch_via_http, url, cfg
            )
            if content:
                method_used = "http"

        if content:
            # LangExtract enhancement for general URLs
            if platform == "general" and LANGEXTRACT_AVAILABLE and len(content) > 300:
                enhanced = await asyncio.get_event_loop().run_in_executor(None, enhance_with_langextract, content, url)
                if enhanced:
                    content = enhanced
                    method_used = f"{method_used}+LE"
            enrichments.append(content)
            summaries.append(f"âœ… {url} â†’ {method_used}")
            logger.info(f"URL è™•ç†æˆåŠŸ: {url} via {method_used}")
        else:
            summaries.append(f"âš ï¸ {url} â†’ ç„¡æ³•æŠ“å–")
            logger.warning(f"URL è™•ç†å¤±æ•—: {url}")

    # çµ„è£å¢å¼·è¨Šæ¯
    if enrichments:
        enriched_block = "\n\n---\n".join(enrichments)
        enhanced_text = (
            f"{text}\n\n"
            f"=== ä»¥ä¸‹æ˜¯è‡ªå‹•æŠ“å–çš„é€£çµå…§å®¹ ===\n\n"
            f"{enriched_block}\n\n"
            f"=== é€£çµå…§å®¹çµæŸ ===\n"
            f"è«‹åŸºæ–¼ä¸Šè¿°é€£çµå…§å®¹ä¾†å›æ‡‰ä½¿ç”¨è€…çš„è¨Šæ¯ã€‚"
        )
        return enhanced_text, summaries

    return text, summaries
